{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用Bagging進行新聞分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,jieba,random\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.ensemble import BaggingClassifier #BaggingClassifier分類模型\n",
    "from sklearn.model_selection import train_test_split #切割訓練與測試資料\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer #提取詞的特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Car News:11740\nTechnology News:25057\nSociety News:268829\nSports News:32728\nFinance News:143141\n\n"
    }
   ],
   "source": [
    "#讀取分類的檔案\n",
    "car_news = pd.read_csv('class_data/car_news.csv',encoding='utf-8')\n",
    "car_news = car_news.dropna()\n",
    "\n",
    "technology_news = pd.read_csv('class_data/technology_news.csv',encoding='utf-8')\n",
    "technology_news = technology_news.dropna()\n",
    "\n",
    "society_news = pd.read_csv('class_data/society_news.csv',encoding='utf-8')\n",
    "society_news = society_news.dropna()\n",
    "\n",
    "sports_news = pd.read_csv('class_data/sports_news.csv',encoding='utf-8')\n",
    "sports_news = sports_news.dropna()\n",
    "\n",
    "finance_news = pd.read_csv('class_data/finance_news.csv',encoding='utf-8')\n",
    "finance_news = finance_news.dropna()\n",
    "\n",
    "print('Car News:{}\\nTechnology News:{}\\nSociety News:{}\\nSports News:{}\\nFinance News:{}\\n'.format(len(car_news),len(technology_news),len(society_news),len(sports_news),len(finance_news)))\n",
    "\n",
    "#每個新聞取出8000筆\n",
    "car_news = car_news[:8000]\n",
    "technology_news = technology_news[:8000]\n",
    "society_news = society_news[:8000]\n",
    "sports_news = sports_news[:8000]\n",
    "finance_news = finance_news[:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list=[]\n",
    "with open('data/stopwords.txt','r',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        stop_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache /var/folders/vk/4wfw6yvn67b0gpkhvj8wy0cw0000gn/T/jieba.cache\nLoading model cost 1.588 seconds.\nPrefix dict has been built successfully.\n"
    }
   ],
   "source": [
    "def preprocess(data,all_data,category):\n",
    "    for line in data:\n",
    "        line = re.sub(r'[^\\w]','',line)\n",
    "        line = re.sub(r'[A-Za-z0-9]','',line)\n",
    "        line = re.sub(u'[\\uFF01-\\uFF5A]','',line)\n",
    "        segment_list = jieba.lcut(line)\n",
    "        segment_list = filter(lambda x: len(x)>1,segment_list)\n",
    "        segment_list = filter(lambda x: x not in stop_list,segment_list)\n",
    "        all_data.append( (' '.join(segment_list),category) )\n",
    "\n",
    "all_data = []\n",
    "preprocess(car_news.content.values.tolist(),all_data,'Car News')\n",
    "preprocess(technology_news.content.values.tolist(),all_data,'Technology News')\n",
    "preprocess(society_news.content.values.tolist(),all_data,'Society News')\n",
    "preprocess(sports_news.content.values.tolist(),all_data,'Sports News')\n",
    "preprocess(finance_news.content.values.tolist(),all_data,'Finance News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_data) #將所有資料打亂順序\n",
    "x,y = zip(*all_data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\n                input='content', lowercase=True, max_df=1.0, max_features=8000,\n                min_df=1, ngram_range=(1, 4), norm='l2', preprocessor=None,\n                smooth_idf=True, stop_words=None, strip_accents=None,\n                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, use_idf=True, vocabulary=None)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# 詞袋模型提取特徵\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word', # 特徵由單詞構成\n",
    "    ngram_range=(1,4), # ngram取1gram到4gram\n",
    "    max_features=8000 # 選最常出現400個單詞構成詞袋\n",
    ")\n",
    "vec.fit(x_train)\n",
    "\n",
    "# TFIDF模型提取特徵\n",
    "tvec = TfidfVectorizer(\n",
    "    analyzer='word', # 特徵由單詞構成\n",
    "    ngram_range=(1,4), # ngram取1gram到4gram\n",
    "    max_features=8000 # 選最常出現400個單詞構成詞袋\n",
    ")\n",
    "tvec.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉驗證\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score\n",
    "\n",
    "def K_Fold_Validation(x_d,y_d,k,classifier):\n",
    "    stratifiedkfold = StratifiedKFold(n_splits=k,shuffle=True)\n",
    "    x_tem = tvec.transform(x_d)\n",
    "    y_tem = np.array(y_d)\n",
    "    y_ref = y_tem[:]\n",
    "    for train_index, test_index in stratifiedkfold.split(x_tem,y_tem):\n",
    "        x_tra, x_tes = x_tem[train_index], x_tem[test_index]\n",
    "        y_tra = y_tem[train_index]\n",
    "        cl = classifier\n",
    "        cl.fit(x_tra,y_tra)\n",
    "        y_ref[test_index] = cl.predict(x_tes)\n",
    "\n",
    "    print('Accuracy: {}'.format( accuracy_score(y_d,y_ref) ))\n",
    "    print('Precision: {}'.format( precision_score(y_d,y_ref,average='macro') ))\n",
    "    print('Recall: {}'.format( recall_score(y_d,y_ref,average='macro') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6903333333333334\nAccuracy: 0.689475\nPrecision: 0.6944319659918803\nRecall: 0.689475\n"
    }
   ],
   "source": [
    "# 訓練並測試模型(提取特徵用TFIDF)\n",
    "bagging_model = BaggingClassifier()\n",
    "bagging_model.fit(tvec.transform(x_train),y_train)\n",
    "print(bagging_model.score(tvec.transform(x_test),y_test))\n",
    "\n",
    "K_Fold_Validation(x,y,5,BaggingClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用Adaboost進行新聞分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.5661666666666667\nAccuracy: 0.549425\nPrecision: 0.6755483396748644\nRecall: 0.549425\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "adaboost_model.fit(tvec.transform(x_train),y_train)\n",
    "print(adaboost_model.score(tvec.transform(x_test),y_test))\n",
    "\n",
    "K_Fold_Validation(x,y,5,adaboost_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用GBDT進行新聞分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6805\nAccuracy: 0.63445\nPrecision: 0.7328213691865656\nRecall: 0.63445\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "GBDT_model = GradientBoostingClassifier()\n",
    "GBDT_model.fit(tvec.transform(x_train),y_train)\n",
    "print(GBDT_model.score(tvec.transform(x_test),y_test))\n",
    "\n",
    "K_Fold_Validation(x,y,5,GBDT_model)"
   ]
  }
 ]
}