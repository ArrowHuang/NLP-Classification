{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基於Tensorflow的CNN訓練文本分類模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,random,jieba\n",
    "import datetime,os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取檔案\n",
    "car_news = pd.read_csv('class_data/car_news.csv',encoding='utf-8')\n",
    "car_news = car_news.dropna()\n",
    "\n",
    "entertainment_news = pd.read_csv('class_data/entertainment_news.csv',encoding='utf-8')\n",
    "entertainment_news = entertainment_news.dropna()\n",
    "\n",
    "international_news = pd.read_csv('class_data/international_news.csv',encoding='utf-8')\n",
    "international_news = international_news.dropna()\n",
    "\n",
    "technology_news = pd.read_csv('class_data/technology_news.csv',encoding='utf-8')\n",
    "technology_news = technology_news.dropna()\n",
    "\n",
    "society_news = pd.read_csv('class_data/society_news.csv',encoding='utf-8')\n",
    "society_news = society_news.dropna()\n",
    "\n",
    "sports_news = pd.read_csv('class_data/sports_news.csv',encoding='utf-8')\n",
    "sports_news = sports_news.dropna()\n",
    "\n",
    "finance_news = pd.read_csv('class_data/finance_news.csv',encoding='utf-8')\n",
    "finance_news = finance_news.dropna()\n",
    "\n",
    "print('Car News:{}\\nEntertainment News:{}\\nInternational News:{}\\nTechnology News:{}\\nSociety News:{}\\nSports News:{}\\nFinance News:{}\\n'.format(len(car_news),len(entertainment_news),len(international_news),len(technology_news),len(society_news),len(sports_news),len(finance_news)))\n",
    "\n",
    "#每個新聞取出11000筆\n",
    "car_news = car_news[:11000]\n",
    "entertainment_news = entertainment_news[:11000]\n",
    "entertainment_news = entertainment_news[:11000]\n",
    "technology_news = technology_news[:11000]\n",
    "society_news = society_news[:11000]\n",
    "sports_news = sports_news[:11000]\n",
    "finance_news = finance_news[:11000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取停用詞\n",
    "stop_list=[]\n",
    "with open('data/stopwords.txt','r',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        stop_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data,all_data,category):\n",
    "    for line in data:\n",
    "        line = re.sub(r'[^\\w]','',line)\n",
    "        line = re.sub(r'[A-Za-z0-9]','',line)\n",
    "        line = re.sub(u'[\\uFF01-\\uFF5A]','',line)\n",
    "        segment = jieba.lcut(line)\n",
    "        segment = filter(lambda x: len(x)>2,segment)\n",
    "        segment = filter(lambda x: x not in stop_list,segment)\n",
    "        all_data.append( (\" \".join(segment),category) )\n",
    "\n",
    "all_data = []\n",
    "preprocess(car_news.content.values.tolist(),all_data,[1,0,0,0,0,0,0])\n",
    "preprocess(technology_news.content.values.tolist(),all_data,[0,1,0,0,0,0,0])\n",
    "preprocess(technology_news.content.values.tolist(),all_data,[0,0,1,0,0,0,0])\n",
    "preprocess(technology_news.content.values.tolist(),all_data,[0,0,0,1,0,0,0])\n",
    "preprocess(society_news.content.values.tolist(),all_data,[0,0,0,0,1,0,0])\n",
    "preprocess(sports_news.content.values.tolist(),all_data,[0,0,0,0,0,1,0])\n",
    "preprocess(finance_news.content.values.tolist(),all_data,[0,0,0,0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN參數(用CPU) Tensorflow\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    CNN文本分類器，主要結構有: a embedding layer + a convolutional, max-pooling and softmax layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
    "                 embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        \"\"\"\n",
    "        :param sequence_length: 句子長度\n",
    "        :param num_classes:     總共有幾個分類類別\n",
    "        :param vocab_size:      詞彙量的大小\n",
    "        :param embedding_size:  詞嵌入的維度\n",
    "        :param filter_sizes:    每個filter處理幾個words\n",
    "        :param num_filters:     每一個filter_sizes的Filter個數\n",
    "        :param l2_reg_lambda:   optional\n",
    "        \"\"\"\n",
    "        \n",
    "        # 定義變數,placeholders佔位符,是tensorflow儲存外部輸入\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name='input_x') #None不限輸入個數\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "        \n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            # 定義一個詞嵌入矩陣,並用隨機均勻分布初始化\n",
    "            self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name='weight')\n",
    "            # 透過查表將input的詞語轉換成詞嵌入,返回[None, sequence_length, embedding_size]\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            # TensorFlow的卷積層conv2d需要四維向量(batch， width，height，channel),增加通道數\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            \n",
    "        \n",
    "        # Convolution + Max-pooling for each filter\n",
    "        pooled_outputs = [] #儲存max pooling之後得到的結果\n",
    "        for i, filter_size in enumerate(filter_sizes): # 利用不同的filter去做convolution得到特徵向量\n",
    "            with tf.name_scope('conv-maxpool-%s' % filter_size):\n",
    "                # convolution layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W') #filter矩陣\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name='b')\n",
    "\n",
    "                \"\"\" embedded_chars_expanded表示詞向量,W表示filter,\n",
    "                    strides表示每次移動步長[1, width, height, 1]\n",
    "                    padding='VALID' 表示不會補充0, padding='SAME' 表示會補充0\n",
    "                \"\"\"\n",
    "                conv = tf.nn.conv2d(self.embedded_chars_expanded, W, strides=[1,1,1,1],\n",
    "                                    padding='VALID', name='conv')\n",
    "                \n",
    "                # activation 激活函數\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
    "                \n",
    "                # max pooling layer\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length-filter_size + 1, 1, 1],\n",
    "                                        strides=[1,1,1,1], padding='VALID', name='pool')\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "                \n",
    "        # 將所有pooling得到的fratures合併起來\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)  \n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        \n",
    "        # Dropout layer防止過擬合\n",
    "        with tf.name_scope('dropout'):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "        \n",
    "        \n",
    "        # 預測得到每個分類的機率\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable('W', shape=[num_filters_total, num_classes],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='b')\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.score = tf.nn.xw_plus_b(self.h_drop, W, b, name='scores')\n",
    "            self.prediction = tf.argmax(self.score, 1, name='prediction')\n",
    "        \n",
    "        \n",
    "        # 計算Loss 最優化對象\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.score, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "           \n",
    "        \n",
    "        # 預測準確度\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(self.prediction, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型參數: 詞向量緯度,每個filter的大小(處理幾個words),每個不同大小的filter個數,dropout機率,L2參數\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "#模型參數: 訓練批次,每一批的訓練資料數目,每多少步測一次,每多少步保存一次模型,保留幾個模型\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# 模型參數: 如果指定GPU沒找到則tensorflow會自動分配, 打印Log日誌 \n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 處理資料部分\n",
    "x,y = zip(*all_data) # 切分訓練與測試資料集\n",
    "max_document_length = 30 # 取設定每句話最長出現文本數\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length) #創建詞彙表\n",
    "x = np.array(list(vocab_processor.fit_transform(x))) \n",
    "\n",
    "# 切分train test validation資料集\n",
    "X_train, x_test, Y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=666) \n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=666) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#產生批次檔\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # 打亂順序\n",
    "        if shuffle:\n",
    "            # 隨機產生一個亂序index\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        # 劃分批次\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開始訓練\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "        log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=7,\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "        timestamp = str(int(time.time()))\n",
    "        # 储存训练好的模型\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"CNN_Result\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # 訓練模型\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "        \n",
    "        # 測試模型，不需要dropout\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "                \n",
    "        # 生成batches\n",
    "        batches = batch_iter(list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        #開始分批次訓練\n",
    "        for batch in batches:\n",
    "            # print batch,把打乱顺序后的、已经分批次的组合数据分开\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_val, y_val, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python36864bitnlpconda2c54c4d051734952b5238f4efd68fcff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
