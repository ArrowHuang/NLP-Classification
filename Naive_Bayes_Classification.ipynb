{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bitnlpconda2c54c4d051734952b5238f4efd68fcff",
   "display_name": "Python 3.6.8 64-bit ('NLP': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用樸素貝葉斯的方法進行分類"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import jieba,xlrd,re\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "兩岸新聞:7343篇, 國際新聞:22976篇, 政治新聞:16153篇, 社會新聞:8366篇, 財經新聞:20665篇\n"
    }
   ],
   "source": [
    "#讀取資料\n",
    "data = pd.read_excel('data/news_data_2019.xlsx',encoding='utf-8')\n",
    "data.dropna() \n",
    "\n",
    "#按照主題儲存新聞\n",
    "chinaAndtw_news = data[ data.name == '兩岸新聞' ]\n",
    "international_news = data[ data.name == '國際新聞' ]\n",
    "political_news = data[ data.name == '政治新聞' ]\n",
    "social_news = data[ data.name == '社會新聞' ]\n",
    "financial_news = data[ data.name == '財經新聞' ]\n",
    "\n",
    "print('兩岸新聞:{}篇, 國際新聞:{}篇, 政治新聞:{}篇, 社會新聞:{}篇, 財經新聞:{}篇'.format(len(chinaAndtw_news),len(international_news),len(political_news),len(social_news),len(financial_news)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "兩岸新聞:7343篇, 國際新聞:7343篇, 政治新聞:7343篇, 社會新聞:7343篇, 財經新聞:7343篇\n"
    }
   ],
   "source": [
    "#對每個新聞隨即抽樣7343確保樣本平均\n",
    "#這邊可能數字太大會跑很慢，可以適當減少\n",
    "chinaAndtw_news = chinaAndtw_news.sample(n=7343,replace=False)\n",
    "international_news = international_news.sample(n=7343,replace=False)\n",
    "political_news = political_news.sample(n=7343,replace=False)\n",
    "social_news = social_news.sample(n=7343,replace=False)\n",
    "financial_news = financial_news.sample(n=7343,replace=False)\n",
    "\n",
    "print('兩岸新聞:{}篇, 國際新聞:{}篇, 政治新聞:{}篇, 社會新聞:{}篇, 財經新聞:{}篇'.format(len(chinaAndtw_news),len(international_news),len(political_news),len(social_news),len(financial_news)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取停用詞\n",
    "stop_list = []\n",
    "with open('data/stopwords.txt','r',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        stop_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache /var/folders/vk/4wfw6yvn67b0gpkhvj8wy0cw0000gn/T/jieba.cache\nLoading model cost 1.507 seconds.\nPrefix dict has been built succesfully.\n"
    }
   ],
   "source": [
    "#斷詞、去停用詞\n",
    "def preprocess(data,all_list,category):\n",
    "    for line in data:\n",
    "        line = re.sub(r'[^\\w]','',line)\n",
    "        line = re.sub(r'[A-Za-z0-9]','',line)\n",
    "        line = re.sub(u'[\\uFF01-\\uFF5A]','',line)\n",
    "        segment_list = jieba.lcut(line)\n",
    "        segment_list = filter(lambda x: len(x)>1,segment_list)\n",
    "        segment_list = filter(lambda x: x not in stop_list,segment_list)\n",
    "        all_data.append((' '.join(segment_list),category))\n",
    "\n",
    "all_data = []\n",
    "preprocess((chinaAndtw_news['title']+chinaAndtw_news['content']).values.tolist(),all_data,'兩岸新聞')\n",
    "preprocess((international_news['title']+international_news['content']).values.tolist(),all_data,'國際新聞')\n",
    "preprocess((chinaAndtw_news['title']+chinaAndtw_news['content']).values.tolist(),all_data,'政治新聞')\n",
    "preprocess((chinaAndtw_news['title']+chinaAndtw_news['content']).values.tolist(),all_data,'社會新聞')\n",
    "preprocess((chinaAndtw_news['title']+chinaAndtw_news['content']).values.tolist(),all_data,'財經新聞')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來是將原始數據進行切割分成訓練資料與測試資料來做訓練與驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將取得的資料全部重洗打亂順序\n",
    "import random\n",
    "random.shuffle(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入scikit-learn來處理\n",
    "from sklearn.model_selection import train_test_split #切分訓練與測試資料\n",
    "from sklearn.feature_extraction.text import CountVectorizer #抽取詞語特徵\n",
    "from sklearn.naive_bayes import MultinomialNB #導入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切割成訓練資料與測試資料\n",
    "x, y = zip(*all_data)\n",
    "x_train, x_test, y_train, y_test  = train_test_split(x,y,test_size=0.3,random_state=666) # 設定相同的random_state可以得到相同的切割結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, vocabulary=None)"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# 將訓練資料中的詞建立詞袋模型提取特徵(轉成向量)\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word', # 特徵由單詞構成\n",
    "    max_features=5000 # 選最常出現5000個單詞構成詞袋\n",
    ")\n",
    "vec.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.2658193372673627"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# 改善前\n",
    "# 讀取模型訓練做訓練並在測試集上面測試效果\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(x_train),y_train) # 訓練\n",
    "classifier.score(vec.transform(x_test),y_test) # 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n                ngram_range=(1, 4), preprocessor=None, stop_words=None,\n                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, vocabulary=None)"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# 改進提取特徵的方法(取更多的ngram以及詞構成詞袋)\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word', # 特徵由單詞構成\n",
    "    ngram_range=(1,4), # ngram取1gram到4gram\n",
    "    max_features=4000 # 選最常出現400個單詞構成詞袋\n",
    ")\n",
    "vec.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.28370403994552884"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# 改善後\n",
    "# 讀取模型訓練做訓練並在測試集上面測試效果\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(x_train),y_train) # 訓練\n",
    "classifier.score(vec.transform(x_test),y_test) # 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.3582731853465886\nPrecision: 0.34007712174807403\nRecall: 0.35827318534658864\n"
    }
   ],
   "source": [
    "# 交叉驗證\n",
    "from sklearn.model_selection import StratifiedKFold # K-Fold Validation\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score #評估準確度\n",
    "\n",
    "stratifiedkfold = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "x_tem = vec.transform(x)\n",
    "y_tem = np.array(y)\n",
    "y_ref = y_tem[:]\n",
    "for train_index,test_index in stratifiedkfold.split(x_tem,y_tem):\n",
    "    x_train,x_test = x_tem[train_index], x_tem[test_index]\n",
    "    y_train = y_tem[train_index]\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(x_train,y_train)\n",
    "    y_ref[test_index] = classifier.predict(x_test)\n",
    "\n",
    "print('Accuracy: {}'.format( accuracy_score(y,y_ref) ))\n",
    "print('Precision: {}'.format( precision_score(y,y_ref,average='macro') ))\n",
    "print('Recall: {}'.format( recall_score(y,y_ref,average='macro') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list=[]\n",
    "with open('data/stopwords.txt','r',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        stop_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Car News:11740\nTechnology News:25057\nSociety News:268829\nSports News:32728\nFinance News:143141\n\n"
    }
   ],
   "source": [
    "#讀取分類的檔案\n",
    "car_news = pd.read_csv('class_data/car_news.csv',encoding='utf-8')\n",
    "car_news = car_news.dropna()\n",
    "\n",
    "technology_news = pd.read_csv('class_data/technology_news.csv',encoding='utf-8')\n",
    "technology_news = technology_news.dropna()\n",
    "\n",
    "society_news = pd.read_csv('class_data/society_news.csv',encoding='utf-8')\n",
    "society_news = society_news.dropna()\n",
    "\n",
    "sports_news = pd.read_csv('class_data/sports_news.csv',encoding='utf-8')\n",
    "sports_news = sports_news.dropna()\n",
    "\n",
    "finance_news = pd.read_csv('class_data/finance_news.csv',encoding='utf-8')\n",
    "finance_news = finance_news.dropna()\n",
    "\n",
    "print('Car News:{}\\nTechnology News:{}\\nSociety News:{}\\nSports News:{}\\nFinance News:{}\\n'.format(len(car_news),len(technology_news),len(society_news),len(sports_news),len(finance_news)))\n",
    "\n",
    "#每個新聞取出8000筆\n",
    "car_news = car_news[:8000]\n",
    "technology_news = technology_news[:8000]\n",
    "society_news = society_news[:8000]\n",
    "sports_news = sports_news[:8000]\n",
    "finance_news = finance_news[:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data,all_data,category):\n",
    "    for line in data:\n",
    "        line = re.sub(r'[^\\w]','',line)\n",
    "        line = re.sub(r'[A-Za-z0-9]','',line)\n",
    "        line = re.sub(u'[\\uFF01-\\uFF5A]','',line)\n",
    "        segement_list = jieba.lcut(line)\n",
    "        segement_list = filter(lambda x: len(x)>1,segement_list)\n",
    "        segement_list = filter(lambda x: x not in stop_list,segement_list)\n",
    "        all_data.append( (' '.join(segement_list),category) )\n",
    "\n",
    "all_data = []\n",
    "preprocess(car_news.content.values.tolist(),all_data,'Car News')\n",
    "preprocess(technology_news.content.values.tolist(),all_data,'Technology News')\n",
    "preprocess(society_news.content.values.tolist(),all_data,'Society News')\n",
    "preprocess(sports_news.content.values.tolist(),all_data,'Sports News')\n",
    "preprocess(finance_news.content.values.tolist(),all_data,'Finance News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#切分資料集\n",
    "x,y = zip(*all_data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=666)\n",
    "\n",
    "#創建貝氏分類器\n",
    "class Bayes_Classification():\n",
    "    def __init__(self, classifier=MultinomialNB()): # 初始化選擇分類模型與詞袋模型\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = CountVectorizer( analyzer='word', ngram_range=(1,4), max_features=4000 )\n",
    "\n",
    "    def features(self,x_train): # 將詞語用詞袋模型轉成向量\n",
    "        return self.vectorizer.transform(x_train)\n",
    "\n",
    "    def fit(self,x_train,y_train): \n",
    "        self.vectorizer.fit(x_train)\n",
    "        self.classifier.fit(self.features(x_train),y_train)\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        return self.classifier.predict(self.features([x_test]))\n",
    "\n",
    "    def score(self,x_test,y_test):\n",
    "        return self.classifier.score(self.features(x_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Car News']\n0.75125\n"
    }
   ],
   "source": [
    "bayes_classifier = Bayes_Classification()\n",
    "bayes_classifier.fit(x_train,y_train)\n",
    "print(bayes_classifier.predict('汽車 很新 好看'))\n",
    "print(bayes_classifier.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}